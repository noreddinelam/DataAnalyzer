{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Handwritten Digit recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The purpose of this model is to develop a  Convolutional Neural Network for recognizing handwritten digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"/home/azureuser/DataAnalyzer/src/modules\")\n",
    "\n",
    "from modules.data_validator import *\n",
    "from modules.data_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Validate the dataset using the data_validator method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "training_path = \"/home/azureuser/DataAnalyzer/digit_data/training\"\n",
    "testing_path =  \"/home/azureuser/DataAnalyzer/digit_data/testing\"\n",
    "for i in range(10):\n",
    "    data_validator(training_path+ str(i))\n",
    "    data_validator(testing_path+ str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate directory to read images from & Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Normalization\n",
    "\n",
    "To avoid overfitting we will alter our dataset.\n",
    "We chose :\n",
    "\n",
    "    - Random rotation by 10Â° of some training image\n",
    "    - Brightness_range from 30 to 70 : The user will take picture so we will adapt our model\n",
    "\n",
    "    -Shift image to make them not in the center\n",
    "        - Randomly shift images vertically\n",
    "        - Randomly shift images horizontally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_datagen = image.ImageDataGenerator(rescale=1.0/255.,rotation_range=10,brightness_range= [30,70], zoom_range = 0.1, # Randomly zoom image\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3) \n",
    "test_datagen = image.ImageDataGenerator(rescale=1.0/255.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Generate and set directory to read images from with data augnemtation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#object of class ImageDataGenerator with the recale property\n",
    "train_generator = train_datagen.flow_from_directory(training_path,batch_size=300,class_mode='sparse',target_size=(28, 28),color_mode=\"rgb\" ,save_to_dir=\"/home/azureuser/digit_data/train\" )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(testing_path, batch_size=300,class_mode='sparse',target_size=(28, 28),color_mode=\"rgb\",save_to_dir=\"/home/azureuser/digit_data/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_generator & train_generator are directory to read images from.\n",
    "Each subdirectory (digits directory) in this directory will be considered to contain images from one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n",
      "/home/azureuser/digit_data/test\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    }
   ],
   "source": [
    "print(test_generator.image_shape)\n",
    "print(test_generator.save_to_dir)\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The CNN modeling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Model : we have to avoid overfitting\n",
    "\n",
    "Softmax :  Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n",
    "activation layer : tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "10 : because we have ten classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 14:59:19.041829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azureuser/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-05-31 14:59:19.041859: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-31 14:59:19.041878: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Data-Ana): /proc/driver/nvidia/version does not exist\n",
      "2022-05-31 14:59:19.042080: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28,3)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### compile the model\n",
    "Following the results got by  https://github.com/sanghvirajit/Feedforward_Neural_Network\n",
    "We make the choice of the RMSprop optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Reduce learning rate when a metric has stopped improving,we will gain in time while fitting the model by converging faster to the global minimum by decreasing the learning rate.\n",
    "Set a learning rate annealer. we chose to decrease by 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                            patience=5, # number of epochs with no improvement after which learning rate will be reduced.\n",
    "                                            verbose=1, #update messages\n",
    "                                            factor=0.25, #new_lr = lr * factor`.\n",
    "                                            min_lr=0.00001 # minimum learning rate\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 23s 449ms/step - loss: 1.8902 - accuracy: 0.3381 - val_loss: 1.3084 - val_accuracy: 0.5679 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 23s 452ms/step - loss: 1.1298 - accuracy: 0.6297 - val_loss: 1.0680 - val_accuracy: 0.6360 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 25s 492ms/step - loss: 0.8368 - accuracy: 0.7341 - val_loss: 0.9332 - val_accuracy: 0.6651 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 22s 440ms/step - loss: 0.6795 - accuracy: 0.7841 - val_loss: 0.7810 - val_accuracy: 0.7145 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 23s 461ms/step - loss: 0.5504 - accuracy: 0.8250 - val_loss: 0.8238 - val_accuracy: 0.6873 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 0.4859 - accuracy: 0.8453 - val_loss: 0.5216 - val_accuracy: 0.8253 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 23s 463ms/step - loss: 0.4249 - accuracy: 0.8673 - val_loss: 0.7545 - val_accuracy: 0.7475 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 27s 540ms/step - loss: 0.4063 - accuracy: 0.8741 - val_loss: 0.4310 - val_accuracy: 0.8530 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 24s 472ms/step - loss: 0.3664 - accuracy: 0.8850 - val_loss: 0.5342 - val_accuracy: 0.8220 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 23s 464ms/step - loss: 0.3432 - accuracy: 0.8913 - val_loss: 0.5117 - val_accuracy: 0.8262 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 24s 491ms/step - loss: 0.3146 - accuracy: 0.9033 - val_loss: 0.7518 - val_accuracy: 0.7564 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 25s 498ms/step - loss: 0.3086 - accuracy: 0.9018 - val_loss: 0.5612 - val_accuracy: 0.8137 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 24s 476ms/step - loss: 0.2824 - accuracy: 0.9080 - val_loss: 0.2749 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.2654 - accuracy: 0.9178 - val_loss: 0.4515 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 26s 517ms/step - loss: 0.2683 - accuracy: 0.9161 - val_loss: 0.4484 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 23s 458ms/step - loss: 0.2521 - accuracy: 0.9223 - val_loss: 0.4085 - val_accuracy: 0.8680 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 23s 466ms/step - loss: 0.2432 - accuracy: 0.9237 - val_loss: 0.3955 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 24s 473ms/step - loss: 0.2339 - accuracy: 0.9276 - val_loss: 0.2115 - val_accuracy: 0.9453 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.2307 - accuracy: 0.9270 - val_loss: 0.2504 - val_accuracy: 0.9257 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.2125 - accuracy: 0.9328 - val_loss: 0.3180 - val_accuracy: 0.9004 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 24s 474ms/step - loss: 0.2102 - accuracy: 0.9323 - val_loss: 0.2180 - val_accuracy: 0.9377 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 24s 474ms/step - loss: 0.2160 - accuracy: 0.9347 - val_loss: 0.3077 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9371\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 23s 466ms/step - loss: 0.2008 - accuracy: 0.9371 - val_loss: 0.2332 - val_accuracy: 0.9331 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 23s 462ms/step - loss: 0.1639 - accuracy: 0.9489 - val_loss: 0.1773 - val_accuracy: 0.9523 - lr: 2.5000e-04\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 23s 463ms/step - loss: 0.1557 - accuracy: 0.9506 - val_loss: 0.1955 - val_accuracy: 0.9444 - lr: 2.5000e-04\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 23s 469ms/step - loss: 0.1546 - accuracy: 0.9523 - val_loss: 0.1977 - val_accuracy: 0.9449 - lr: 2.5000e-04\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.1543 - accuracy: 0.9526 - val_loss: 0.1740 - val_accuracy: 0.9538 - lr: 2.5000e-04\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 23s 469ms/step - loss: 0.1558 - accuracy: 0.9519 - val_loss: 0.1630 - val_accuracy: 0.9568 - lr: 2.5000e-04\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 24s 490ms/step - loss: 0.1516 - accuracy: 0.9523 - val_loss: 0.1636 - val_accuracy: 0.9557 - lr: 2.5000e-04\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 0.1476 - accuracy: 0.9541 - val_loss: 0.1655 - val_accuracy: 0.9560 - lr: 2.5000e-04\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 23s 469ms/step - loss: 0.1460 - accuracy: 0.9541 - val_loss: 0.1955 - val_accuracy: 0.9424 - lr: 2.5000e-04\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 0.1547 - accuracy: 0.9509 - val_loss: 0.1967 - val_accuracy: 0.9433 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9513\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 23s 457ms/step - loss: 0.1482 - accuracy: 0.9513 - val_loss: 0.1770 - val_accuracy: 0.9500 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 26s 498ms/step - loss: 0.1356 - accuracy: 0.9575 - val_loss: 0.1693 - val_accuracy: 0.9527 - lr: 6.2500e-05\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 23s 469ms/step - loss: 0.1420 - accuracy: 0.9555 - val_loss: 0.1820 - val_accuracy: 0.9468 - lr: 6.2500e-05\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 23s 468ms/step - loss: 0.1418 - accuracy: 0.9555 - val_loss: 0.1895 - val_accuracy: 0.9435 - lr: 6.2500e-05\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.1437 - accuracy: 0.9541 - val_loss: 0.1676 - val_accuracy: 0.9535 - lr: 6.2500e-05\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9569\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.1356 - accuracy: 0.9569 - val_loss: 0.1634 - val_accuracy: 0.9552 - lr: 6.2500e-05\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 24s 480ms/step - loss: 0.1309 - accuracy: 0.9571 - val_loss: 0.1600 - val_accuracy: 0.9563 - lr: 1.5625e-05\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.1305 - accuracy: 0.9583 - val_loss: 0.1685 - val_accuracy: 0.9533 - lr: 1.5625e-05\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 23s 465ms/step - loss: 0.1375 - accuracy: 0.9567 - val_loss: 0.1629 - val_accuracy: 0.9554 - lr: 1.5625e-05\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 24s 473ms/step - loss: 0.1383 - accuracy: 0.9569 - val_loss: 0.1608 - val_accuracy: 0.9562 - lr: 1.5625e-05\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9547\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "50/50 [==============================] - 24s 471ms/step - loss: 0.1451 - accuracy: 0.9547 - val_loss: 0.1666 - val_accuracy: 0.9534 - lr: 1.5625e-05\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 24s 476ms/step - loss: 0.1273 - accuracy: 0.9597 - val_loss: 0.1658 - val_accuracy: 0.9536 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 23s 461ms/step - loss: 0.1339 - accuracy: 0.9576 - val_loss: 0.1646 - val_accuracy: 0.9543 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 25s 489ms/step - loss: 0.1370 - accuracy: 0.9570 - val_loss: 0.1640 - val_accuracy: 0.9547 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 23s 466ms/step - loss: 0.1343 - accuracy: 0.9569 - val_loss: 0.1643 - val_accuracy: 0.9545 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.1375 - accuracy: 0.9561 - val_loss: 0.1616 - val_accuracy: 0.9553 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.1293 - accuracy: 0.9567 - val_loss: 0.1600 - val_accuracy: 0.9562 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.1326 - accuracy: 0.9568 - val_loss: 0.1609 - val_accuracy: 0.9560 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, epochs=50, steps_per_epoch=50,\n",
    "        validation_data=test_generator,  callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABDklEQVR4nO2TO4uFMBCFnYmSqPgAH4WVCIKF////iI2VFtcHMZrcYuEW69Vlc2Gb9ZTnMB+HzMQwbv29AIAQAgC/GLnIEDHPc0rpsiyUUill0zTrun5UsCzLOI5fThiGdV0HQaAP9X0/TdNvpmmaVVWFYagJTZKEMXb0AaAoCkLIxSyeBVJKIcTRV0oNw+B5ng4UEc82Pk2TZVk6UKUUpfRttO/7tm06UM75WR1E1HxTIYRpmm8jSqlSSge6bdtZ0zRNx3HUhALAcVd5ns/zzDnXgRqGIaV8lUVEx3GqquKcd113MWVc/33P877uMcsyxljf923bPh6Pa+IPQsSyLKMosm3bdd2PWLdu/Q89AaJgUvwmUaNJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n",
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/azureuser/DataAnalyzer/src/Tensorflow/digits.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B20.223.151.80/home/azureuser/DataAnalyzer/src/Tensorflow/digits.ipynb#ch0000038vscode-remote?line=0'>1</a>\u001b[0m evaluate_digits_model(\u001b[39m\"\u001b[39;49m\u001b[39m/home/azureuser/DataAnalyzer/digit_data/validation_statique/my0.png\u001b[39;49m\u001b[39m\"\u001b[39;49m,model)\n",
      "File \u001b[0;32m~/DataAnalyzer/src/modules/data_processing.py:54\u001b[0m, in \u001b[0;36mevaluate_digits_model\u001b[0;34m(img_path, model)\u001b[0m\n\u001b[1;32m     <a href='file:///home/azureuser/DataAnalyzer/src/modules/data_processing.py?line=51'>52</a>\u001b[0m img\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='file:///home/azureuser/DataAnalyzer/src/modules/data_processing.py?line=52'>53</a>\u001b[0m black_and_white(img)\n\u001b[0;32m---> <a href='file:///home/azureuser/DataAnalyzer/src/modules/data_processing.py?line=53'>54</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mresize((\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m))\n\u001b[1;32m     <a href='file:///home/azureuser/DataAnalyzer/src/modules/data_processing.py?line=54'>55</a>\u001b[0m img\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='file:///home/azureuser/DataAnalyzer/src/modules/data_processing.py?line=55'>56</a>\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(img)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "evaluate_digits_model(\"/home/azureuser/DataAnalyzer/digit_data/validation_statique/my0.png\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
